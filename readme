# ğŸ§  AI Test Case Generator

> Automatically generate high-quality test cases from natural-language requirements using an LLaVA-based local model.
> Includes real-time CPU/GPU monitoring, structured outputs, and customizable prompt templates.

---

## ğŸš€ Overview

The **AI Test Case Generator** streamlines the process of converting human-written software requirements into structured test cases.
It connects to a **local LLaVA model** (via Ollama or API endpoint), generates test cases in batch or parallel mode, and monitors your systemâ€™s performance during execution.

---

## âœ¨ Features

* ğŸ§© **AI-Powered Generation** â€” Uses the LLaVA model for intelligent, context-aware test creation.
* âš™ï¸ **Batch / Parallel Modes** â€” Generate multiple test cases at once or concurrently.
* ğŸ“Š **Resource Monitoring** â€” Tracks CPU, memory, and GPU usage in real time.
* ğŸ¦¾ **Structured Output** â€” Automatically saves generated test cases in the `outputs/` directory.
* ğŸ§  **Prompt Customization** â€” Easily edit prompt templates under the `prompts/` folder.
* ğŸ’œ **Logging** â€” Logs CPU/GPU stats in `cpu_usage.log` for performance insights.

---

## ğŸ—‚ï¸ Folder Structure

```
project-root/
â”œâ”€â”€ main.py                  # Entry point for the app
â”œâ”€â”€ cpu_usage.log            # Logs system performance metrics
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ settings.py          # Global configuration and model settings
â”‚   â””â”€â”€ logger.py            # Logging setup
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ model_client.py      # Handles API communication with LLaVA model
â”‚   â”œâ”€â”€ system_monitor.py    # Monitors CPU, memory, and GPU
â”‚   â”œâ”€â”€ test_case_generator.py
â”‚   â”œâ”€â”€ test_case_parser.py
â”‚   â”œâ”€â”€ prompt_loader.py
â”‚   â””â”€â”€ utils.py
â”œâ”€â”€ prompts/                 # Input prompt templates
â”œâ”€â”€ outputs/                 # Generated test cases
â”œâ”€â”€ requirements/            # Requirement samples (used as input)
â””â”€â”€ requirements.txt         # Python dependencies
```

---

## âš™ï¸ Installation

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/<your-username>/ai-test-case-generator.git
cd ai-test-case-generator
```

### 2ï¸âƒ£ Create and Activate a Virtual Environment

```bash
python -m venv env
source env/bin/activate        # macOS / Linux
env\Scripts\activate           # Windows
```

### 3ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

---

## ğŸ¤– Model Setup (LLaVA via Ollama)

This project assumes you are running **Ollama** locally with the **LLaVA** model.

1. Install [Ollama](https://ollama.ai/download)
2. Pull the model:

   ```bash
   ollama pull llava:7b
   ```
3. Run Ollama:

   ```bash
   ollama run llava:7b
   ```
4. Ensure your API endpoint matches:

   ```
   http://localhost:11434/api/generate
   ```

---

## ğŸ§© Usage

### Run the main script

```bash
python main.py
```

The script will:

* Import a sample `requirement` from `requirements/sample_requirement.py`
* Start system monitoring
* Generate test cases
* Save the results inside the `outputs/` folder
* Log resource usage to `cpu_usage.log`

---

## ğŸ“Š Logs and Monitoring

The system monitor runs in the background and records:

* CPU usage percentage
* Memory usage
* GPU stats (if available via `pynvml`)

You can inspect the log with:

```bash
cat cpu_usage.log
```

---

## ğŸ§  Configuration

| Setting         | Description                        | Default                               |
| --------------- | ---------------------------------- | ------------------------------------- |
| `MODEL_API_URL` | API endpoint for model requests    | `http://localhost:11434/api/generate` |
| `MODEL_NAME`    | Model name to use                  | `llava:7b`                            |
| `USE_STREAM`    | Enable streaming responses         | `True`                                |
| `OUTPUT_DIR`    | Directory for generated test cases | `./outputs`                           |

Edit these in **`config/settings.py`**.

---

## ğŸ§© Example Output

Each generated test case file in `outputs/` may include:

```json
{
  "id": "testcase_001",
  "requirement": "User should be able to log in with valid credentials.",
  "steps": [
    "Open the login page",
    "Enter valid username and password",
    "Click Login",
    "Verify successful navigation to dashboard"
  ],
  "expected_result": "User is logged in successfully."
}
```

---

## ğŸ§® Tech Stack

* **Python 3.10+**
* **Requests** â€“ API communication
* **Psutil + Pynvml** â€“ System resource monitoring
* **LLaVA / Ollama** â€“ AI model for test generation

---

## ğŸ¤ Contributing

Contributions, issues, and feature requests are welcome!
Feel free to open a PR or raise an issue.

---

## ğŸ“„ License

This project is released under the [MIT License](LICENSE).

---

## ğŸ‘¨â€ğŸ’» Author

**Your Name**
ğŸ”— [GitHub](https://github.com/<your-username>)
ğŸ’¼ [LinkedIn](https://linkedin.com/in/<your-profile>)

---

> â­ **If you find this project useful, please consider starring the repo!**
